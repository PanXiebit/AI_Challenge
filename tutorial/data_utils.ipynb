{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import jieba\n",
    "import tqdm\n",
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dady 's car\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4kgs  kg '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = re.sub(r\"(\\w)'s\\b\", r\"\\1 's\", \"dady's car\") \n",
    "print(sent)\n",
    "re.sub(r\"(\\d+)kgs \", lambda m: m.group() + ' kg ', \"4kgs \") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 英文文本正则化预处理\n",
    "def simple_token(sent):\n",
    "    # 在正则表达式内部，可以用\\n引用括号匹配的内容，n是从1开始的自然数，表示对应顺序的括号\n",
    "    sent = re.sub(r\"(\\w)'s\\b\", r\"\\1 is\", sent)    # make 's a separate word\n",
    "    sent = re.sub(r\"(\\w[’'])(\\w)\", r\"\\1 \\2\", sent) # other ' in a word creates 2 words\n",
    "    sent = re.sub(r\"([\\\"().,;:/_?!-])\", r\" \\1 \", sent).replace('-',' ')    # add spaces around punctuation\n",
    "    sent = re.sub(r\"  *\", r\" \", sent)               # replace multiple spaces with just one\n",
    "    return sent.lower()\n",
    "\n",
    "### 中文文本正则化处理\n",
    "def zh_token(sent):\n",
    "    sent = re.sub(r\"\")\n",
    "\n",
    "class MySentence():\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in tqdm.tqdm(open(self.dirname)):\n",
    "            yield simple_token(line)\n",
    "            \n",
    "class MyZhSentence():\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in tqdm.tqdm(open(self.dirname)):\n",
    "            yield list(jieba.cut(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a pair of red crowned cranes have staked out their nesting territory\n",
      "\n",
      "a pair of crows had come to nest on our roof as if they had come for lhamo . \n",
      "\n",
      "a couple of boys driving around in daddy is car . \n",
      "\n",
      "a pair of nines ? you pushed in with a pair of nines ? \n",
      "\n",
      "fighting two against one is never ideal , \n",
      "\n",
      "it is a neat one two . walker to burton . \n",
      "\n",
      "deuces the winner . \n",
      "\n",
      "five on one . five on one . yeah , not the greatest odds . \n",
      "\n",
      "an incredibly emotional fight between 2 sisters？\n",
      "\n",
      "one against 500 . \n",
      "\n",
      "pair of fives , the winner . \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "en_file = \"train/train.en\"\n",
    "en_corpus = MySentence(en_file)\n",
    "for i,sent in enumerate(en_corpus):\n",
    "    if i > 10:\n",
    "        break\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.411 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "1it [00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一对', '丹顶鹤', '正', '监视', '着', '它们', '的', '筑巢', '领地']\n",
      "['一对', '乌鸦', '飞', '到', '我们', '屋顶', '上', '的', '巢里', '，', '它们', '好像', '专门', '为拉木', '而', '来', '的', '。']\n",
      "['一对', '乖乖仔', '开着', '老爸', '的', '车子', '。']\n",
      "['一对', '九', '？', '一对', '九', '你', '就', '全', '下注', '了', '？']\n",
      "['一对二', '总', '不是', '好事', '，']\n",
      "['一对二', '，', '沃克', '传给', '波顿', '。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zh_file = \"train/train.zh\"\n",
    "zh_corpus = MyZhSentence(zh_file)\n",
    "for i,sent in enumerate(zh_corpus):\n",
    "    if i > 5:\n",
    "        break\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_wordvec(lan=\"en\", embedding_dim = 300):\n",
    "    output = 'vocab/' + lan\n",
    "    if lan == 'en':\n",
    "        corpus = MySentence('train/train.en')\n",
    "    else:\n",
    "        corpus = MyZhSentence(\"train/train.zh\")\n",
    "    model = Word2Vec(corpus, size=embedding_dim, min_count=10, sg=1)\n",
    "    print(lan + \"model trained\")\n",
    "    \n",
    "    # 词表\n",
    "    vocabulary = model.wv.vocab  # dict\n",
    "    words = list(vocabulary.keys())\n",
    "    \n",
    "    # 开头、结尾、padding、未知词\n",
    "    function_words = ['<pading>','<start>','<end>','<unk>']\n",
    "    words = function_words + words    \n",
    "    word2index = dict(zip(words, range(len(words))))\n",
    "    \n",
    "    input_dim = len(words)\n",
    "    embeddings = []\n",
    "    for word in vocabulary:\n",
    "        embeddings.append(model[word])\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "    \n",
    "    weights = np.zeros((input_dim, embedding_dim), np.float32)\n",
    "    weights[1] = np.ones(embedding_dim, np.float32) * 0.33   # start\n",
    "    weights[2] = np.ones(embedding_dim, np.float32) * 0.66   # end\n",
    "    weights[3] = np.average(embeddings, axis = 0)\n",
    "    weights[4:] = embeddings\n",
    "    \n",
    "    # 保存词表\n",
    "    pickle.dump(word2index, open(output + '_vocab.pkl', 'wb'))\n",
    "    # 保存词向量\n",
    "    np.save(output + '_word2vec.npy', embeddings)\n",
    "    print(lan + '_word vector saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练好的词向量怎么评价质量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mknod(\"en_vocab.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
